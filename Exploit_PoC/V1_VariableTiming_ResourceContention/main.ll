; ModuleID = 'main.c'
source_filename = "main.c"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@global_variable = dso_local global i64 15, align 8
@array = dso_local global [262144 x i8] zeroinitializer, align 512
@value = dso_local global double 0.000000e+00, align 8
@dummy = dso_local global i32 0, align 4
@start = dso_local global i64 0, align 8
@end = dso_local global i64 0, align 8
@.str = private unnamed_addr constant [12 x i8] c"Timing %ld\0A\00", align 1
@tmp = dso_local global i64 0, align 8
@slow = dso_local global <2 x double> zeroinitializer, align 16
@fast = dso_local global <2 x double> zeroinitializer, align 16
@result = dso_local global <2 x double> zeroinitializer, align 16

; Function Attrs: noinline nounwind optnone uwtable
define dso_local double @victim_function(double %0, i32 %1) #0 {
  %3 = alloca double, align 8
  %4 = alloca double, align 8
  %5 = alloca i32, align 4
  %6 = alloca double, align 8
  %7 = alloca i32, align 4
  store double %0, double* %4, align 8
  store i32 %1, i32* %5, align 4
  store volatile i32 0, i32* %7, align 4
  br label %8

8:                                                ; preds = %12, %2
  %9 = load volatile i32, i32* %7, align 4
  %10 = icmp slt i32 %9, 200
  br i1 %10, label %11, label %15

11:                                               ; preds = %8
  br label %12

12:                                               ; preds = %11
  %13 = load volatile i32, i32* %7, align 4
  %14 = add nsw i32 %13, 1
  store volatile i32 %14, i32* %7, align 4
  br label %8, !llvm.loop !4

15:                                               ; preds = %8
  call void asm sideeffect "lfence;\0Amfence;\0Asfence", "~{dirflag},~{fpsr},~{flags}"() #2, !srcloc !6
  %16 = load i32, i32* %5, align 4
  %17 = load i8, i8* getelementptr inbounds ([262144 x i8], [262144 x i8]* @array, i64 0, i64 2048), align 512
  %18 = zext i8 %17 to i32
  %19 = icmp slt i32 %16, %18
  br i1 %19, label %20, label %117

20:                                               ; preds = %15
  %21 = fmul double %0, %0
  
  %22 = call double @llvm.sqrt.f64(double %0)
%23 = fmul double %22, %22
%24 = call double @llvm.sqrt.f64(double %23)
%25 = fmul double %24, %24
%26 = call double @llvm.sqrt.f64(double %25)
%27 = fmul double %26, %26
%28 = call double @llvm.sqrt.f64(double %27)
%29 = fmul double %28, %28
%30 = call double @llvm.sqrt.f64(double %29)
%31 = fmul double %30, %30
%32 = call double @llvm.sqrt.f64(double %31)
%33 = fmul double %32, %32
%34 = call double @llvm.sqrt.f64(double %33)
%35 = fmul double %34, %34
%36 = call double @llvm.sqrt.f64(double %35)
%37 = fmul double %36, %36
%38 = call double @llvm.sqrt.f64(double %37)
%39 = fmul double %38, %38
%40 = call double @llvm.sqrt.f64(double %39)
%41 = fmul double %40, %40
%42 = call double @llvm.sqrt.f64(double %41)
%43 = fmul double %42, %42
%44 = call double @llvm.sqrt.f64(double %43)
%45 = fmul double %44, %44
%46 = call double @llvm.sqrt.f64(double %45)
%47 = fmul double %46, %46
%48 = call double @llvm.sqrt.f64(double %47)
%49 = fmul double %48, %48
%50 = call double @llvm.sqrt.f64(double %49)
%51 = fmul double %50, %50
%52 = call double @llvm.sqrt.f64(double %51)
%53 = fmul double %52, %52
%54 = call double @llvm.sqrt.f64(double %53)
%55 = fmul double %54, %54
%56 = call double @llvm.sqrt.f64(double %55)
%57 = fmul double %56, %56
%58 = call double @llvm.sqrt.f64(double %57)
%59 = fmul double %58, %58
%60 = call double @llvm.sqrt.f64(double %59)
%61 = fmul double %60, %60
%62 = call double @llvm.sqrt.f64(double %61)
%63 = fmul double %62, %62
%64 = call double @llvm.sqrt.f64(double %63)
%65 = fmul double %64, %64
%66 = call double @llvm.sqrt.f64(double %65)
%67 = fmul double %66, %66
%68 = call double @llvm.sqrt.f64(double %67)
%69 = fmul double %68, %68
%70 = call double @llvm.sqrt.f64(double %69)
%71 = fmul double %70, %70
%72 = call double @llvm.sqrt.f64(double %71)
%73 = fmul double %72, %72
%74 = call double @llvm.sqrt.f64(double %73)
%75 = fmul double %74, %74
%76 = call double @llvm.sqrt.f64(double %75)
%77 = fmul double %76, %76
%78 = call double @llvm.sqrt.f64(double %77)
%79 = fmul double %78, %78
%80 = call double @llvm.sqrt.f64(double %79)
%81 = fmul double %80, %80
%82 = call double @llvm.sqrt.f64(double %81)
%83 = fmul double %82, %82
%84 = call double @llvm.sqrt.f64(double %83)
%85 = fmul double %84, %84
%86 = call double @llvm.sqrt.f64(double %85)
%87 = fmul double %86, %86
%88 = call double @llvm.sqrt.f64(double %87)
%89 = fmul double %88, %88
%90 = call double @llvm.sqrt.f64(double %89)
%91 = fmul double %90, %90
%92 = call double @llvm.sqrt.f64(double %91)
%93 = fmul double %92, %92
%94 = call double @llvm.sqrt.f64(double %93)
%95 = fmul double %94, %94
%96 = call double @llvm.sqrt.f64(double %95)
%97 = fmul double %96, %96
%98 = call double @llvm.sqrt.f64(double %97)
%99 = fmul double %98, %98
%100 = call double @llvm.sqrt.f64(double %99)
%101 = fmul double %100, %100
%102 = call double @llvm.sqrt.f64(double %101)
%103 = fmul double %102, %102
%104 = call double @llvm.sqrt.f64(double %103)
%105 = fmul double %104, %104
%106 = call double @llvm.sqrt.f64(double %105)
%107 = fmul double %106, %106
%108 = call double @llvm.sqrt.f64(double %107)
%109 = fmul double %108, %108
%110 = call double @llvm.sqrt.f64(double %109)
%111 = fmul double %110, %110
%112 = call double @llvm.sqrt.f64(double %111)
%113 = fmul double %112, %112

%114 = call double @llvm.sqrt.f64(double %113)
%115 = fmul double %114, %114

  ; Save this for return the value
  store double %115, double* %6, align 8
%116 = load volatile i64, i64* @global_variable, align 8

  br label %117

117:                                               ; preds = %20, %15
  ;%118 = load double, double* %3, align 8
  %118 = load double, double* %6, align 8
  ret double %118
}

; Function Attrs: noinline nounwind optnone uwtable
define dso_local i32 @main() #0 {
  %1 = alloca i32*, align 8
  %2 = alloca i32*, align 8
  %3 = alloca i32, align 4
  %4 = alloca i64, align 8
  %5 = alloca i64, align 8
  %6 = alloca double, align 8
  %7 = alloca double, align 8
  %8 = alloca double, align 8
  %9 = alloca i32, align 4
  %10 = alloca double, align 8
  store i32 0, i32* %3, align 4
  store i64 4748437441418039, i64* %4, align 8
  store i64 4679240012837945344, i64* %5, align 8
  store double 0.000000e+00, double* %6, align 8
  call void asm sideeffect "movq $0, %xmm3", "*m,~{dirflag},~{fpsr},~{flags}"(i64* %5) #2, !srcloc !87
  call void asm sideeffect "sqrtsd %xmm3, %xmm4", "~{dirflag},~{fpsr},~{flags}"() #2, !srcloc !88
  call void asm sideeffect "movq %xmm3, $0", "*m,~{dirflag},~{fpsr},~{flags}"(double* %7) #2, !srcloc !89
  call void asm sideeffect "movq $0, %xmm3", "*m,~{dirflag},~{fpsr},~{flags}"(i64* %4) #2, !srcloc !90
  call void asm sideeffect "sqrtsd %xmm3, %xmm4", "~{dirflag},~{fpsr},~{flags}"() #2, !srcloc !91
  call void asm sideeffect "movq %xmm3, $0", "*m,~{dirflag},~{fpsr},~{flags}"(double* %8) #2, !srcloc !92
  %11 = load double, double* %7, align 8 ; Load FAST
  ;%11 = load double, double* %8, align 8  ; Load SLOW
  store double %11, double* @value, align 8
  call void asm sideeffect "lfence;\0Amfence;\0Asfence", "~{dirflag},~{fpsr},~{flags}"() #2, !srcloc !93
  store i32 0, i32* %9, align 4
  br label %12

12:                                               ; preds = %20, %0
  %13 = load i32, i32* %9, align 4
  %14 = sext i32 %13 to i64
  %15 = icmp ult i64 %14, 262144
  br i1 %15, label %16, label %23

16:                                               ; preds = %12
  %17 = load i32, i32* %9, align 4
  %18 = sext i32 %17 to i64
  %19 = getelementptr inbounds [262144 x i8], [262144 x i8]* @array, i64 0, i64 %18
  store i8 0, i8* %19, align 1
  br label %20

20:                                               ; preds = %16
  %21 = load i32, i32* %9, align 4
  %22 = add nsw i32 %21, 1
  store i32 %22, i32* %9, align 4
  br label %12, !llvm.loop !94

23:                                               ; preds = %12
  store i8 10, i8* getelementptr inbounds ([262144 x i8], [262144 x i8]* @array, i64 0, i64 2048), align 512
  call void asm sideeffect "lfence;\0Amfence;\0Asfence", "~{dirflag},~{fpsr},~{flags}"() #2, !srcloc !95
  call void asm sideeffect "clflush 0($0)", "r,~{dirflag},~{fpsr},~{flags}"(i8* getelementptr inbounds ([262144 x i8], [262144 x i8]* @array, i64 0, i64 0)) #2, !srcloc !96
  call void asm sideeffect "lfence;\0Amfence;\0Asfence", "~{dirflag},~{fpsr},~{flags}"() #2, !srcloc !97
  %24 = load double, double* %6, align 8
  %25 = call double @victim_function(double %24, i32 0)
  %26 = load double, double* %6, align 8
  %27 = call double @victim_function(double %26, i32 0)
  call void asm sideeffect "lfence;\0Amfence;\0Asfence", "~{dirflag},~{fpsr},~{flags}"() #2, !srcloc !98
  call void asm sideeffect "clflush 0($0)", "r,~{dirflag},~{fpsr},~{flags}"(i8* getelementptr inbounds ([262144 x i8], [262144 x i8]* @array, i64 0, i64 2048)) #2, !srcloc !99
  call void asm sideeffect "clflush 0($0)", "r,~{dirflag},~{fpsr},~{flags}"(i64* @global_variable) #2, !srcloc !100
  call void asm sideeffect "lfence;\0Amfence;\0Asfence", "~{dirflag},~{fpsr},~{flags}"() #2, !srcloc !101
  %28 = load double, double* @value, align 8
  %29 = call double @victim_function(double %28, i32 100)
  store double %29, double* %10, align 8
  call void asm sideeffect "lfence;\0Amfence;\0Asfence", "~{dirflag},~{fpsr},~{flags}"() #2, !srcloc !102
  store i32* @dummy, i32** %1, align 8
  %30 = load i32*, i32** %1, align 8
  %31 = call { i64, i32 } @llvm.x86.rdtscp() #2
  %32 = extractvalue { i64, i32 } %31, 1
  store i32 %32, i32* %30, align 4
  %33 = extractvalue { i64, i32 } %31, 0
  store i64 %33, i64* @start, align 8
  %34 = load volatile i8, i8* bitcast (i64* @global_variable to i8*), align 8
  store i32* @dummy, i32** %2, align 8
  %35 = load i32*, i32** %2, align 8
  %36 = call { i64, i32 } @llvm.x86.rdtscp() #2
  %37 = extractvalue { i64, i32 } %36, 1
  store i32 %37, i32* %35, align 4
  %38 = extractvalue { i64, i32 } %36, 0
  %39 = load i64, i64* @start, align 8
  %40 = sub i64 %38, %39
  store i64 %40, i64* @end, align 8
  call void asm sideeffect "lfence;\0Amfence;\0Asfence", "~{dirflag},~{fpsr},~{flags}"() #2, !srcloc !103
  %41 = load i64, i64* @end, align 8
  %42 = call i32 (i8*, ...) @printf(i8* getelementptr inbounds ([12 x i8], [12 x i8]* @.str, i64 0, i64 0), i64 %41)
  %43 = load i32, i32* %3, align 4
  ret i32 %43
}

declare double @llvm.sqrt.f64(double) #1

declare dso_local i32 @printf(i8*, ...) #2

; Function Attrs: nounwind
declare { i64, i32 } @llvm.x86.rdtscp() #3

attributes #0 = { noinline nounwind optnone uwtable "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #1 = { "frame-pointer"="all" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #2 = { nounwind }

!llvm.module.flags = !{!0, !1, !2}
!llvm.ident = !{!3}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"uwtable", i32 1}
!2 = !{i32 7, !"frame-pointer", i32 2}
!3 = !{!"clang version 14.0.0 (https://github.com/llvm/llvm-project.git 27451a05ed4d13294182ec7e999a9d4f90bc0d12)"}
!4 = distinct !{!4, !5}
!5 = !{!"llvm.loop.mustprogress"}
!6 = !{i64 2150545296, i64 2150545306, i64 2150545315}
!7 = !{i64 911}
!8 = !{i64 955}
!9 = !{i64 999}
!10 = !{i64 1043}
!11 = !{i64 1087}
!12 = !{i64 1131}
!13 = !{i64 1175}
!14 = !{i64 1219}
!15 = !{i64 1263}
!16 = !{i64 1307}
!17 = !{i64 1351}
!18 = !{i64 1395}
!19 = !{i64 1439}
!20 = !{i64 1483}
!21 = !{i64 1527}
!22 = !{i64 1571}
!23 = !{i64 1615}
!24 = !{i64 1659}
!25 = !{i64 1703}
!26 = !{i64 1747}
!27 = !{i64 1791}
!28 = !{i64 1835}
!29 = !{i64 1879}
!30 = !{i64 1923}
!31 = !{i64 1967}
!32 = !{i64 2011}
!33 = !{i64 2055}
!34 = !{i64 2099}
!35 = !{i64 2143}
!36 = !{i64 2187}
!37 = !{i64 2231}
!38 = !{i64 2275}
!39 = !{i64 2319}
!40 = !{i64 2363}
!41 = !{i64 2407}
!42 = !{i64 2451}
!43 = !{i64 2495}
!44 = !{i64 2539}
!45 = !{i64 2583}
!46 = !{i64 2627}
!47 = !{i64 2671}
!48 = !{i64 2715}
!49 = !{i64 2759}
!50 = !{i64 2803}
!51 = !{i64 2847}
!52 = !{i64 2891}
!53 = !{i64 2935}
!54 = !{i64 2979}
!55 = !{i64 3023}
!56 = !{i64 3067}
!57 = !{i64 3111}
!58 = !{i64 3155}
!59 = !{i64 3199}
!60 = !{i64 3243}
!61 = !{i64 3287}
!62 = !{i64 3331}
!63 = !{i64 3375}
!64 = !{i64 3419}
!65 = !{i64 3463}
!66 = !{i64 3507}
!67 = !{i64 3551}
!68 = !{i64 3595}
!69 = !{i64 3639}
!70 = !{i64 3683}
!71 = !{i64 3727}
!72 = !{i64 3771}
!73 = !{i64 3815}
!74 = !{i64 3859}
!75 = !{i64 3903}
!76 = !{i64 3947}
!77 = !{i64 3991}
!78 = !{i64 4035}
!79 = !{i64 4079}
!80 = !{i64 4123}
!81 = !{i64 4167}
!82 = !{i64 4211}
!83 = !{i64 4255}
!84 = !{i64 4299}
!85 = !{i64 4343}
!86 = !{i64 4387}
!87 = !{i64 4626}
!88 = !{i64 4679}
!89 = !{i64 4719}
!90 = !{i64 4771}
!91 = !{i64 4824}
!92 = !{i64 4864}
!93 = !{i64 2150545382, i64 2150545392, i64 2150545401}
!94 = distinct !{!94, !5}
!95 = !{i64 2150545430, i64 2150545440, i64 2150545449}
!96 = !{i64 2150545485}
!97 = !{i64 2150545556, i64 2150545566, i64 2150545575}
!98 = !{i64 2150545599, i64 2150545609, i64 2150545618}
!99 = !{i64 2150545654}
!100 = !{i64 2150545739}
!101 = !{i64 2150545804, i64 2150545814, i64 2150545823}
!102 = !{i64 2150545847, i64 2150545857, i64 2150545866}
!103 = !{i64 2150545890, i64 2150545900, i64 2150545909}
